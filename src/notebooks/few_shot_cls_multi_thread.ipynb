{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5ceee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinjiewu/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from queue import Queue, Empty\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "pd.set_option('display.max_rows', None) # Force to display all rows of tables\n",
    "#pd.set_option('display.max_rows', 20)\n",
    "\n",
    "from dataset_utility import img_col_map, label_col_map, get_metadata_row, filter_one_condition_scin_clinical\n",
    "from gemini_api import GeminiAPIHandler\n",
    "from cls_generation import call_gemini_api_cls\n",
    "from cls_eval import eval_gemini_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02304659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "dataset = \"derm12345\"\n",
    "#dataset = \"bcn20000\"\n",
    "#dataset = \"hiba\"\n",
    "#dataset = \"pad-ufes-20\"\n",
    "#dataset = \"scin_clinical\"\n",
    "dataset_path = f\"../../datasets/{dataset}\"\n",
    "\n",
    "run_cls = True\n",
    "#run_cls = False\n",
    "\n",
    "isMultiStep = False\n",
    "if dataset == \"derm12345\":\n",
    "    #isMultiStep = False\n",
    "    isMultiStep = True\n",
    "\n",
    "use_context = True\n",
    "#use_context = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c773dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(dataset_path, \"metadata.csv\")\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "label_col = label_col_map[dataset]\n",
    "img_col = img_col_map[dataset]\n",
    "\n",
    "output_dir = f\"../api_cls/{dataset}\"\n",
    "if isMultiStep: \n",
    "    output_dir = output_dir + \"_multistep\"\n",
    "os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "\n",
    "log_filename = os.path.join(output_dir, \"api_clscont.log\")\n",
    "os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "    filename=log_filename\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c963f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataset directory\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path, topdown=False):\n",
    "        if dirpath == dataset_path:\n",
    "            continue  # Skip the root itself\n",
    "\n",
    "        for file in filenames:\n",
    "            src_path = os.path.join(dirpath, file)\n",
    "            dest_path = os.path.join(dataset_path, file)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "        # Optionally remove the now-empty subfolders\n",
    "        os.rmdir(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b289736",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "api_key_indices = [8, 9, 10]\n",
    "#api_key_indices = [1, 2, 3]\n",
    "api_handler_queue = Queue()\n",
    "\n",
    "model_name = \"gemini-2.0-flash-exp\"\n",
    "request_interval = 6\n",
    "#model_name = \"gemini-2.0-flash-thinking-exp\"\n",
    "#request_interval = 12\n",
    "\n",
    "for index in api_key_indices:\n",
    "    api_key = os.getenv(f\"GEMINI_API_KEY_{index}\")\n",
    "    api_handler = GeminiAPIHandler(\n",
    "        api_key=api_key, \n",
    "        index=index, \n",
    "        model_name=model_name, \n",
    "        request_interval=request_interval, \n",
    "        logger=logger\n",
    "    )\n",
    "    api_handler_queue.put(api_handler)\n",
    "\n",
    "# Multi-thread configuration\n",
    "NUM_WORKERS = len(api_handler_queue.queue)\n",
    "TIMEOUT_BOUND = 300\n",
    "HANDLER_COOLDOWN = 60\n",
    "\n",
    "# Shared structures\n",
    "worker_queues = [Queue() for _ in range(NUM_WORKERS)]\n",
    "results = []\n",
    "results_lock = threading.Lock()\n",
    "worker_threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1dfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_timeout(worker_id, old_handler):\n",
    "    try:\n",
    "        new_handler = api_handler_queue.get_nowait()\n",
    "        logger.info(f\"[Worker_{worker_id}] Swapping to API_Handler_{new_handler.index}.\")\n",
    "        api_handler_queue.put(old_handler)\n",
    "        return new_handler\n",
    "    except Empty:\n",
    "        logger.info(f\"[Worker_{worker_id}] No other available handler. Sleeping {HANDLER_COOLDOWN}s before retry...\")\n",
    "        time.sleep(HANDLER_COOLDOWN)\n",
    "        return old_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83babc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"scin_clinical\":\n",
    "    # Filter the images with only one dignosis label\n",
    "    metadata = filter_one_condition_scin_clinical(metadata)\n",
    "elif dataset in {\"derm12345\", \"bcn20000\", \"hiba\", \"pad-ufes-20\"}:\n",
    "    # Filter the images with non-empty label\n",
    "    metadata = metadata[metadata[label_col].replace(r'^\\s*$', pd.NA, regex=True).notna()]\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset name. Please provide a valid dataset name.\")\n",
    "\n",
    "img_ids = [f.removesuffix('.jpg').removesuffix('.png') for f in metadata[img_col].dropna().unique()]\n",
    "\n",
    "# Distribute image IDs across worker queues\n",
    "for i, image_id in enumerate(img_ids):\n",
    "    worker_queues[i % NUM_WORKERS].put(image_id)\n",
    "\n",
    "# Parse all existing response files in the output directory\n",
    "old_response_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
    "for filename in old_response_files:\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with open(file_path, 'r') as f:\n",
    "        stored_result = json.load(f)\n",
    "    stored_result_df = pd.DataFrame([stored_result])\n",
    "    results.append(stored_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3891df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  33%|███▎      | 809/2485 [00:00<00:00, 6124895.19img/s]"
     ]
    }
   ],
   "source": [
    "# Thread-safe progress bar\n",
    "progress_bar = tqdm(total=len(img_ids), desc=\"Processing images\", unit=\"img\")\n",
    "progress_bar.n = len(old_response_files)\n",
    "progress_bar.refresh()\n",
    "progress_lock = threading.Lock()\n",
    "\n",
    "def update_progress_bar(task_exists, progress_made, last_update):\n",
    "    elapsed_time = time.time() - last_update\n",
    "    if not task_exists and progress_made:\n",
    "        last_update = time.time()\n",
    "        with progress_lock:\n",
    "            progress_bar.update(1)\n",
    "            if progress_bar.n % 50 == 0:\n",
    "                current_results = []\n",
    "                with results_lock: current_results = results[:]\n",
    "                current_results_df = pd.concat(current_results, ignore_index=True)\n",
    "                eval_gemini_cls(current_results_df, dataset, isMultiStep, isprocessing=True)\n",
    "    return last_update, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88f1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_fn(worker_id, api_handler, stop_event):\n",
    "    queue_ = worker_queues[worker_id - 1]\n",
    "    last_update = time.time()\n",
    "\n",
    "    try:\n",
    "        while not queue_.empty() and not stop_event.is_set():\n",
    "            image_id = None\n",
    "            try:\n",
    "                image_id = queue_.get_nowait()\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "            task_exists = False\n",
    "            progress_made = False\n",
    "\n",
    "            try:\n",
    "                metadata_row = get_metadata_row(metadata, dataset, image_id)\n",
    "                generated_response_file = os.path.join(output_dir, f\"{image_id}_cls.json\")\n",
    "\n",
    "                if os.path.exists(generated_response_file):\n",
    "                    logger.info(f\"[Worker_{worker_id}] Skipping image_id {image_id} as response already exists.\")\n",
    "                    task_exists = True\n",
    "                else:\n",
    "                    result = call_gemini_api_cls(api_handler, dataset, image_id, metadata_row, isMultiStep, use_context)\n",
    "                    result_df = pd.DataFrame([result])\n",
    "                    with results_lock: results.append(result_df)\n",
    "\n",
    "                    with open(generated_response_file, 'w') as out_file:\n",
    "                        json.dump(result, out_file, indent=4)\n",
    "\n",
    "                    logger.info(f\"[Worker_{worker_id}] Successfully processed {image_id} \" + \n",
    "                                f\"using Handler_{api_handler.index}.\")\n",
    "                    progress_made = True\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[Worker_{worker_id}] Error processing {image_id} \" + \n",
    "                               f\"using Handler_{api_handler.index}: {e}\")\n",
    "                if image_id:\n",
    "                    queue_.put(image_id)\n",
    "\n",
    "            finally:\n",
    "                queue_.task_done()\n",
    "                last_update, elapsed_time = update_progress_bar(task_exists, progress_made, last_update)\n",
    "                if elapsed_time > TIMEOUT_BOUND:\n",
    "                    logger.info(f\"[Worker_{worker_id}] Handler_{api_handler.index} timeout. \" + \n",
    "                                f\"Elapsed time: {elapsed_time:.2f}s.\")\n",
    "                    api_handler = handle_timeout(worker_id, api_handler)\n",
    "                    last_update = time.time()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(f\"[Worker_{worker_id}] Received KeyboardInterrupt. Shutting down...\")\n",
    "\n",
    "    finally:\n",
    "        api_handler_queue.put(api_handler)\n",
    "        logger.info(f\"[Worker_{worker_id}] Finished all tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17052325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  33%|███▎      | 809/2485 [07:27<15:27,  1.81img/s]     \n"
     ]
    }
   ],
   "source": [
    "def run_all():\n",
    "    stop_event = threading.Event()  # Shared event to signal shutdown\n",
    "    worker_threads.clear()  # Reset thread list in case this is re-run\n",
    "\n",
    "    # Start all worker threads\n",
    "    for i in range(1, NUM_WORKERS + 1):\n",
    "        try:\n",
    "            api_handler = api_handler_queue.get_nowait()\n",
    "        except Empty:\n",
    "            logger.error(\"Not enough API handlers available for workers.\")\n",
    "            break\n",
    "\n",
    "        t = threading.Thread(target=worker_fn, args=(i, api_handler, stop_event), daemon=True)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "\n",
    "    try:\n",
    "        # Wait for all threads to complete\n",
    "        for t in worker_threads:\n",
    "            t.join()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"KeyboardInterrupt received. Signaling workers to stop...\")\n",
    "        stop_event.set()\n",
    "        for t in worker_threads:\n",
    "            t.join()\n",
    "\n",
    "    progress_bar.close()\n",
    "    logger.info(\"All workers completed.\")\n",
    "\n",
    "\n",
    "if run_cls:\n",
    "    run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5126c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(output_dir, \"evaluation_metrics\")\n",
    "if os.path.exists(eval_dir):\n",
    "    shutil.rmtree(eval_dir) # Delete the old evaluation file folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1431728",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_response_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
    "print(f\"{len(all_response_files)}/{len(img_ids)} of the images have been processed so far.\\n\")\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "if not results_df.empty:\n",
    "    eval_gemini_cls(results_df, dataset, isMultiStep, isprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Clean up new response files to roll back if needed\n",
    "new_response_files = [f for f in all_response_files if f not in old_response_files]\n",
    "for filename in new_response_files:\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    os.remove(file_path)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
