{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from dataset_utility import label_col_map, filter_one_condition_scin_clinical\n",
    "\n",
    "# Create output folder\n",
    "# dataset = \"derm12345\"\n",
    "dataset = \"scin_clinical\" # Get a service account key before running this\n",
    "# dataset = \"hiba\"\n",
    "output_folder = f\"../../datasets/{dataset}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "metadata_path = os.path.join(output_folder, \"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def connect_to_gcs(gcp_project, bucket_name):\n",
    "    \"\"\"\n",
    "    Connect to Google Cloud Storage and return the bucket.\n",
    "    \"\"\"\n",
    "    # Path to your service account key file\n",
    "    key_path = \"/path/to/your/service-account-key.json\"\n",
    "\n",
    "    # Create credentials object\n",
    "    credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "    # Client for querying GCS\n",
    "    client = storage.Client(credentials=credentials, project=gcp_project) \n",
    "\n",
    "    # Bucket object for loading files\n",
    "    bucket = client.bucket(bucket_name) \n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(row, prefix):\n",
    "# Merge the columns with the same prefix\n",
    "    combined = []\n",
    "    for col in row.keys():\n",
    "        if col.startswith(f\"{prefix}_\") and row[col] == \"YES\":\n",
    "            combined.append(col[len(f\"{prefix}_\"):])\n",
    "    return ','.join(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "if dataset == \"scin\":\n",
    "    gcs_bucket = connect_to_gcs(\"dx-scin-public\", \"dx-scin-public-data\")\n",
    "    \n",
    "    # Merge the metadata CSV\n",
    "    cases_df = pd.read_csv(io.BytesIO(gcs_bucket.blob(\"dataset/scin_cases.csv\").download_as_string()), dtype={'case_id': str})\n",
    "    labels_df = pd.read_csv(io.BytesIO(gcs_bucket.blob(\"dataset/scin_labels.csv\").download_as_string()), dtype={'case_id': str})\n",
    "    cases_df['case_id'] = cases_df['case_id'].astype(str)\n",
    "    labels_df['case_id'] = labels_df['case_id'].astype(str)\n",
    "    cases_and_labels_df = pd.merge(cases_df, labels_df, on='case_id')\n",
    "\n",
    "    # Merge the columns with the same prefix\n",
    "    for field in ['race_ethnicity', 'textures', 'body_parts', 'condition_symptoms', 'other_symptoms']:\n",
    "        cases_and_labels_df[field] = cases_and_labels_df.apply(lambda row: merge_columns(row, field), axis=1)\n",
    "        cases_and_labels_df.drop(columns=[col for col in cases_and_labels_df.columns if col.startswith(f\"{field}_\")], \n",
    "                                 inplace=True)\n",
    "\n",
    "    cases_and_labels_df['symptoms'] = cases_and_labels_df['condition_symptoms'].str.cat(cases_and_labels_df['other_symptoms'], \n",
    "                                                                                        sep=',', na_rep='')\n",
    "    cases_and_labels_df.drop(columns=['condition_symptoms', 'other_symptoms'], inplace=True)\n",
    "\n",
    "    # Rename the columns 'image_1_shot_type',... to match the match the format for pd.wide_to_long()\n",
    "    cases_and_labels_df.rename(columns=lambda col: re.sub(r'image_(\\d+)_(.+)', r'image_\\2_\\1', col), inplace=True)\n",
    "\n",
    "    # Unpivot the image fields\n",
    "    metadata_df = pd.wide_to_long(\n",
    "        cases_and_labels_df,\n",
    "        stubnames=['image_path', 'image_shot_type', 'dermatologist_gradable_for_skin_condition',\n",
    "                   'dermatologist_gradable_for_fitzpatrick_skin_type', 'dermatologist_fitzpatrick_skin_type_label'],\n",
    "        i=[col for col in cases_and_labels_df.columns if not re.search(r'_\\d+$', col)],  # Remaining columns that are not numbered\n",
    "        j='image_index',\n",
    "        sep='_',\n",
    "        suffix='\\\\d+'\n",
    "    ).reset_index() # Retain the remaining columns\n",
    "\n",
    "    # Remove rows with NaN image paths\n",
    "    metadata_df = metadata_df.dropna(subset=['image_path'])\n",
    "\n",
    "    # Iterate over the image_path column and download each image\n",
    "    for image_path in metadata_df['image_path']:\n",
    "        if isinstance(image_path, str):  # Ensure the path is valid\n",
    "            # Download the image from the GCS bucket\n",
    "            blob = gcs_bucket.blob(image_path)\n",
    "\n",
    "            # Extract the image filename\n",
    "            image_filename = os.path.basename(image_path)\n",
    "            local_image_path = os.path.join(output_folder, image_filename)\n",
    "\n",
    "            # Check if the image already exists locally\n",
    "            if os.path.exists(local_image_path):\n",
    "                continue\n",
    "\n",
    "            with open(local_image_path, 'wb') as image_file:\n",
    "                image_data = blob.download_as_bytes()\n",
    "\n",
    "                # Convert the image data to an RGB PIL Image and save it as JPEG\n",
    "                image = Image.open(io.BytesIO(image_data))\n",
    "                image.convert(\"RGB\").save(local_image_path, format=\"JPEG\")\n",
    "    \n",
    "    metadata_df['image_path'] = metadata_df['image_path'].apply(os.path.basename)\n",
    "    metadata_df.to_csv(metadata_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "if dataset == \"derm12345\":\n",
    "    # Login using e.g. `huggingface-cli login` to access this dataset\n",
    "    # derm12345 = load_dataset(\"abdurrahimyilmaz/derm12345_synthetic_data\", split=\"train\")\n",
    "    derm12345 = load_dataset(\"abdurrahimyilmaz/derm12345_synthetic_data\", split=\"test\")\n",
    "    for sample in derm12345:\n",
    "        image = sample['image']\n",
    "        image_id = sample['image_id']\n",
    "\n",
    "        # Save image as JPG\n",
    "        image_path = os.path.join(output_folder, image_id)\n",
    "        image.convert(\"RGB\").save(image_path, format=\"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Generate taxonomy file for the HIBA/SCIN dataset\n",
    "if dataset == \"hiba\" or dataset == \"scin_clinical\":\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    label_col = label_col_map[dataset]\n",
    "    if dataset == \"scin_clinical\": metadata_df = filter_one_condition_scin_clinical(metadata_df)\n",
    "    classes = sorted(metadata_df[label_col].dropna().unique())\n",
    "    print(classes)\n",
    "    taxonomy_path = os.path.join(output_folder, \"taxonomy.json\")\n",
    "    with open(taxonomy_path, 'w') as f:\n",
    "        json.dump(classes, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
